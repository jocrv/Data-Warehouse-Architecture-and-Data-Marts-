ETL Process - Transformation and Data Loading of Fact Tables
Presentation
In this class, the steps of transformation and loading of the Transactional, aggregated and consolidated fact tables will be presented; the concept of data purging, the creation of the JOB in the PDI tool and the concept of process management.

Goals
Describe the step of data transformation and application of business rules;
Examine the types of transformations applied to data in the ETL process;
Identify the data loading step in the definitive fact tables.
ETL Process - Loading of Transactional, Aggregated and Consolidated Fact Tables
In the previous class, the ETL process was described and the ETL tool was presented, which helps the development of the process of extracting, transforming and loading data. In this class, we will continue the concepts learned and proceed with the transformation of the Transaction Fact table, aggregated data, and consolidated data.

Fact table load
The Fact table stores measurable business data such as quantities and amounts. The fact that occurred is described by the views that make up the DW/DM. Thus, in addition to measurable data, the table stores the relationship with the dimensions through the primary key (SK) of each table.
know more
In order for the Fact table to be loaded successfully, the dimension keys must be validated. This means that the SK key of the dimension to be inserted in the Fact table must exist in the dimension. For that, the keys must be verified and added to the temporary table of the Fact table, so that the validated records are selected and inserted in it.

The following figure illustrates the steps taken to extract data from the source database table to the Temporary Fact table that records sales.

Extracting and loading the temporary sales fact table



The Read Table Input step accesses the source database and selects the data contained in the operating system's sales table. In our example, we can just use a simple query that will return the data we need:
SELECT * FROM saleproducts;

However, in a real project, the business rules must be observed so that only the records referring to sales made in the desired period are extracted (Day, Month, among others).

The Read Sequence Sales (AddSequence) step generates the SK key for the temporary table; the Load Temp Sales (Table Output) step inserts the records extracted from the source system into the Temporary Fact table and the step Validates Null Codes assigns a value of 0 to the null keys of the elements that will be validated in the dimensions, according to the following command:

  
UPDATE tmp_ft_sales SET cd_cliente_sales_tmp = 0

WHERE id_ft_sale_tmp IN (SELECT id_ft_sale_tmp

FROM tmp_ft_salesft

WHERE ft.cd_customer_sales_tmp ISNULL);

Thus, the sales records in which the customer code is equal to null, will receive the value 0 (zero) and, at the end of this step, the records are loaded in the temporary fact table and ready for the dimensions' primary key validations.

See the validation steps of the SKs keys of the dimensions.
The Read SK from Dimensions (Table Output) step selects the dimension keys through the element code in the source system.

After validation and filling in the dimension codes, the dimension keys must be validated and updated in the temporary table in the Sales Fact table.

The figure below illustrates the query contained in the Select SKs Dimensions (Table Input) step, which relates the temporary table of the Sales Fact table with the dimensions through the element codes. Note that since only the Product temp table has the Category key and it is linked directly to the Fact table so the sk key was retrieved using the tmp_product table.
Validation of SK keys of dimensions.

After selecting the SKs, they must be updated in the temporary table.
Update of the SKs keys in the Sales Temporary Fact table.
Comment
Other validations and transformations can be applied at this stage as needed for the data being treated. With the data ready to be inserted into the final Fact table, it must be selected and inserted into the Fact table.

See the Select Sales (Table Input) step, which selects the records to be inserted into the Fact table. If the temporary table contains a column indicating that the record is free to be loaded into the Fact table, the condition must be respected when selecting rows, as well as other forms of restrictions on data validation. In our example, all lines will be loaded.
Sales Fact Table Load.

Next, we see the selection of the fields contained in the Temporary Fact table. The following command was generated by the Get SQL statement button. However, here you can select only the fields to be inserted in the Final Fact table.
Selection of data for the Sales Fact Table.
Finally, the Insert Sales (Table Output) step inserts the records into the Sales Fact table. Note that only fields to be inserted into the Fact table are mapped to this step.
Selection of data for the Sales Fact Table. See the ft_sales table loaded with the supermarket sales records.
Sales Fact Table.
Now, to exercise the verified steps, build the transformation for the Inventory Fact table.

Aggregate Sales Fact Table Load
In previous lessons you learned about data aggregation, where the Aggregate Fact table stores precalculated information according to the level of granularity you want for the analyzes for which the Aggregate Fact table is being built, which is higher than the from the Transactional Fact table.

Heads up
According to this concept, when preparing the data for the aggregated fact table, the data must be summarized and the level of granularity reduced, as is the case in the following example.

The Aggregate Sales Fact table is intended to support the analysis of products sold in the grain month. The Date dimension has all the days of the month, but since we're going to load the Fact table aggregated into the month grain, we must choose a single record from the Date dimension that represents the month. For example, it will be the first day of the month. The quantity of products sold measure must be summarized and the sales value must be calculated based on the measures quantity of products and value of the product sold.

Below is the example illustrated below.

Sales Fact Table aggregation query.
To examine the result of the aggregation query, click on the preview button and the result illustrated in the figure below will be displayed. Note that the result shows the sum of the product quantity and the total sale value per product and per month.

Aggregate query result.
To complete the transformation, a step for inserting data (Table Output) in the aggregated Fact table must be added. In the step Insert Aggregate Sales, list the query fields with the fields from the Aggregate Fact table (agr_sales_product).
Aggregate fact table load. 
After the transformation runs, the Aggregate Sales Fact table is loaded.

Consolidated Fact Table Load
The Consolidated Fact table aggregates data by joining data contained in more than one Fact table, for example, the Sales Fact table and the Inventory Fact table from the Supermarket scenario.

In the Supermarket scenario, there is an analysis that requires the data to be consolidated, to answer the question: which are the manufacturers of the products that offer the greatest profit in the sale of their items?

Tip
To answer this question it is necessary to relate the data of the product purchased from the manufacturer and the data of the product sold to customers. The desired metrics should be calculated using the metrics contained in the Inventory Fact table and the Sales Fact table.

As we have an aggregate of sales in the grain month, it will be used in the consolidation.
Data Consolidation Command Profit Product
The image below illustrates the result of the data consolidation and which will be inserted in the Final Fact table
Consolidated Profit Product data.
To complete the transformation, the step for inserting the data (Table Output) in the Aggregated Fact Profit/Product table must be added. In the Insert Consolidated Profit Product step, list the query fields with the fields from the aggregated Fact table (agr_profit_product).
Consolidated Profit Product data.
At this point, the transformations for loading the data into DW Supermercado are ready. Now, a new concept will be introduced for understanding how data is archived with low or no frequency of use.
Data Purge
Data purging consists of removing data from the DW/DM database that are no longer accessed or rarely accessed. Generally, a time is defined in which the data are stored and, from that date, the data is collected and stored in other repositories or media that will only be used if there is a need to back up the information.

If there is a need to perform the data purge, a transformation must be created that indicates which data should be archived and the period that should be purged.
After creating the transformations, it's time to create the JOB that will chain the tasks to be executed and that allows you to schedule the process so that it runs on certain days and times.

The Job must always start with step Start. In it, it is possible to define the days and times of execution of the process. Look.

DW Supermarket ETL Process
Next, the Transformation steps must be added, which point to each transformation created. These must be placed in the execution priority order, especially when the data contained in a table needs to be validated in another table, for example, the load of the Fact table needs to validate the SKs of the dimensions that make up the sale. Transformations that do not depend on the execution of another transformation can be paralleled, such as the Category, Customer, and Date transformations.

The Figure illustrates an example of a step transformation that points to the file of the transformation that must be executed.
Step Transformation Category.
After the completion of the JOB, it can be run to process the entire chain of data extraction, transformation and loading processes.
Process management
Kimball (2013) describes in his book that a DW/DM project needs to offer reliability, availability and manageability: reliability ensures that processes will be executed consistently; availability must ensure that the environment is ready to use when needed, and manageability, with respect to growing the compliant environment, supports the reliability of the environment and its availability.

know more
As mentioned before, the ETL process needs to be orchestrated by a scheduler that will define the moment when the process will start and that will start each next task, obeying the predecessor's execution. If the order of tasks is not respected, possible errors can be presented and the main objective of the entire project, which is the availability of the analytical environment, will not be achieved.

In this class, we verify the steps of extraction, treatment and loading of Transactional Fact, aggregated and consolidated tables; we saw what data purging is for and we talked about process management.

Now, based on the applied concepts, let's fix the understanding!
Loading data into a Fact table:

a) Must consider descriptions of all dimension elements to ensure data integrity.
b) It must happen before the dimensions so that there are no referential integrity problems.
c) Insert SK keys before loading measurements to ensure referential integrity.
d) It happens after loading the dimensions and validates the SK keys of each dimension so that there are no referential integrity problems.
e) It does not have validations, as all data were validated in the data extraction step.
D. The table stores measurable data and is related to the dimensions that describe it. Therefore, it must be loaded after loading the dimensions and the SK keys must be validated so that data integrity is not violated.
The Data Warehouse (DW) data purge:

a) It must be carried out after completing one year of information to consolidate and archive the data, always keeping the last year available for consultation.
b) It should not include historical data with less than 10 years of storage.
c) Does not apply to any subject of a DW.
d) It must not occur, as it may affect the analyzes carried out in the DW.
e) The period of data removal must be determined by the organization and the removed data must be stored on media that allows recovering the data if necessary.
E. Data purge is a process of archiving historical data that is rarely accessed or no longer used in analytics. Generally, low access media are used and data is retrieved, in case there is a need to consult data from the archived period. The purge process must be created and defined as needed by the organization.
The process management process defined by Kimball (2013) is composed of three points to be checked:

a) Reliability, availability and manageability.
b) Processing, availability and manageability.
c) Reliability, Validation and manageability.
d) Reliability, Availability and Checking.
e) Standardization, availability and manageability.
A. Process management is about keeping the DW environment reliable, available, and manageable to support the organization's analytics and meet changing needs after the project closes.
